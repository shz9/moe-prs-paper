{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mixture of Experts (MoE) for Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47a929f4b5afd859"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we will explore how to use the Mixture of Experts (MoE) framework to fit pre-trained \n",
    "linear models to a simple 1 dimensional dataset. We will compare the MoE model to a simple linear model \n",
    "that follows the `MultiPRS` framework (i.e. use all pre-trained models as covariates). \n",
    "\n",
    "To start, let's import the necessary libraries and modules that will be needed for this notebook.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea04a71ec8a68003"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../model/\")\n",
    "from PRSDataset import PRSDataset\n",
    "from moe import MoEPRS\n",
    "from baseline_models import MultiPRS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:09:55.157254Z",
     "start_time": "2024-02-21T23:09:55.076264Z"
    }
   },
   "id": "2b97c36ebb4a9f22",
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8276a91232438287"
  },
  {
   "cell_type": "markdown",
   "source": [
    "After importing the necessary libraries and modules, we will create a simple 1 dimensional dataset \n",
    "that displays heterogeneity in the relationship between the input variable `x` and the output variable `y`. The \n",
    "easiest way to do this would be to split the input domain in two halves, and then in each division \n",
    "generate data according to a linear model with a different slope:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46f84a3a53df6a8d"
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(7209)\n",
    "\n",
    "N = 10_000  # sample size\n",
    "x = np.random.normal(size=N)  # Generate x from a normal distribution\n",
    "arg_sort_x = np.argsort(x)  # Sort x (for plotting purposes)\n",
    "y = np.empty_like(x)  # Pre-allocate the output (dependent variable y)\n",
    "\n",
    "b1 = 1.  # The slope for the first half of the domain\n",
    "b2 = -1.  # The slope for the second half of the domain\n",
    "\n",
    "# Generate the deterministic part of the output\n",
    "# Here, we split the domain in two halves at 0 exactly:\n",
    "y[x < 0.] = x[x < 0.]*b1\n",
    "y[x >= 0.] = x[x >= 0.]*b2\n",
    "\n",
    "# Add some noise to the output\n",
    "y += np.random.normal(scale=0.5, size=N)\n",
    "\n",
    "# Plot the data:\n",
    "plt.scatter(x[arg_sort_x], y[arg_sort_x], color='#D3D3D3', marker='.')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:09:55.531181Z",
     "start_time": "2024-02-21T23:09:55.122300Z"
    }
   },
   "id": "7593d6af6e0222dc",
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot above shows the generated data. As we can see, the relationship between `x` and `y` is not linear in general,\n",
    "however, it can be well approximated by linear models locally (within the two halves of the domain that we defined before).\n",
    "\n",
    "## Pre-trained Linear Models\n",
    "\n",
    "To make this setup more equivalent to the polygenic score (PGS) setting, let's assume that we are given access to a number \n",
    "of pre-trained linear models. Let's further assume that, similar to GWAS/PGS settings, those models were pre-trained on \n",
    "different subsets or domains of the data. To do that, we will implement the following procedure:\n",
    "\n",
    "1. Split the data into two halves (choose splitting point randomly from a uniform between -1 and 1).\n",
    "2. Train a linear model on each half of the data.\n",
    "3. To make things interesting, we'll include a bunch of random linear models as well that are unrelated to the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c1c9d5e5a23a744"
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the random seed:\n",
    "np.random.seed(7209)\n",
    "\n",
    "# Sample the split point:\n",
    "split_point = np.random.uniform(-1, 1)\n",
    "\n",
    "# Split x and y based on x's value with respect to the split point:\n",
    "x1_mask = x < split_point  # Define a mask for points in the first half of the domain\n",
    "\n",
    "# Extract points in the first half of the domain:\n",
    "x1 = x[x1_mask]\n",
    "y1 = y[x1_mask]\n",
    "\n",
    "# Extract points in the second half of the domain:\n",
    "x2 = x[~x1_mask]\n",
    "y2 = y[~x1_mask]\n",
    "\n",
    "# Train a linear model on each half of the domain:\n",
    "b1_hat = np.cov(x1, y1, ddof=0)[0, 1] / np.var(x1)\n",
    "b2_hat = np.cov(x2, y2, ddof=0)[0, 1] / np.var(x2)\n",
    "\n",
    "# Generate some random linear models:\n",
    "n_random_models = 3\n",
    "\n",
    "# Generate random linear models:\n",
    "random_b_hats = np.random.normal(scale=1, size=n_random_models)\n",
    "\n",
    "# Put the beta hats together:\n",
    "b_hats = np.concatenate([[b1_hat], [b2_hat], random_b_hats])\n",
    "\n",
    "# Generate the predictions of the pre-trained models:\n",
    "preds = x.reshape(-1, 1).dot(b_hats.reshape(1, -1))\n",
    "# Generate the colors for the pre-trained models:\n",
    "clrs = sns.color_palette('gist_ncar', n_colors=preds.shape[1])\n",
    "\n",
    "# Plot the data and the pre-trained models:\n",
    "plt.scatter(x[arg_sort_x], y[arg_sort_x], color='#D3D3D3', marker='.')\n",
    "\n",
    "for i in range(preds.shape[1]):\n",
    "    if i < 2:\n",
    "        label = f\"Model {i+1}\"\n",
    "    else:\n",
    "        label = f\"Random Model {i-1}\"\n",
    "    plt.plot(x[arg_sort_x], preds[arg_sort_x, i], color=clrs[i], label=label)\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "# Plot the split point:\n",
    "plt.axvline(split_point, color='r', linestyle='--', label='Training split point')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:09:56.098411Z",
     "start_time": "2024-02-21T23:09:55.528563Z"
    }
   },
   "id": "1403925415857c92",
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that in this case, because of the randomness in splitting the data, the split point is not exactly at 0. This makes \n",
    "the second model (Model 2) fit the data a bit poorly, nevertheless, it still fits the data better than Model 1 (or any of the \n",
    "other models for that matter!).\n",
    "\n",
    "## Meta models\n",
    "\n",
    "Now that we've generated the pre-trained models, we can use them to fit a meta model. A meta PRS model is a model that takes the \n",
    "predictions of simpler models and combines them in such a way that the combined model fits the data better than any of the\n",
    "simpler models individually. Before we explore the meta PRS models, let's aggregate our data in a `PRSDataset` object:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a9c86e5e38e3f02"
  },
  {
   "cell_type": "code",
   "source": [
    "# Aggregate the data + model predictions in a dataframe:\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "model_names = []\n",
    "\n",
    "for i in range(preds.shape[1]):\n",
    "    \n",
    "    if i < 2:\n",
    "        model_name = f\"Model_{i+1}\"\n",
    "    else:\n",
    "        model_name = f\"Random_Model_{i-1}\"\n",
    "    \n",
    "    df[model_name] = preds[:, i]\n",
    "    model_names.append(model_name)\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:09:56.120295Z",
     "start_time": "2024-02-21T23:09:56.098859Z"
    }
   },
   "id": "fc7f143a7df3836f",
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a PRSDataset object:\n",
    "\n",
    "prs_data = PRSDataset(df, \n",
    "                      phenotype_col='y', # Specify the target column (equivalent to phenotype)\n",
    "                      prs_cols=model_names, # Specify the column names containing pre-trained model predictions\n",
    "                      covariates_cols='x') # Specify the column names containing the covariates (if any)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:09:56.227277Z",
     "start_time": "2024-02-21T23:09:56.118448Z"
    }
   },
   "id": "e37f0d786bdf18c0",
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MultiPRS\n",
    "\n",
    "First, we will explore the MultiPRS model formulation, which takes the individual predictions of the pre-trained\n",
    "models and combines them linearly to get the final prediction:\n",
    "\n",
    "$$\n",
    "y_{MultiPRS}(i) = \\alpha_0 + \\sum_{k=1}^{K} \\hat{\\beta}_k \\hat{y}_k(i) \n",
    "$$\n",
    "\n",
    "where $\\hat{y}_k(i)$ is the prediction of the $k$-th pre-trained model for the $i$-th observation, and $\\hat{\\beta}_k$ is the\n",
    "coefficient of the $k$-th pre-trained model. The coefficient $\\alpha_0$ is the intercept of the meta model.\n",
    "\n",
    "Here's how the MultiPRS model can be fit to the data using the `MultiPRS` class from the `baseline_models` module:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c755e85f2eb07e"
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the MultiPRS model:\n",
    "multi_prs = MultiPRS(prs_data,\n",
    "                     expert_cols=model_names, # Specify the column names containing pre-trained model predictions\n",
    "                     covariates_cols='x')\n",
    "\n",
    "# Fit the MultiPRS model:\n",
    "multi_prs.fit()\n",
    "\n",
    "# Visualize the fitted model predictions:\n",
    "multi_prs_pred = multi_prs.predict()\n",
    "\n",
    "# Plot the data and the pre-trained models:\n",
    "plt.scatter(x[arg_sort_x], y[arg_sort_x], color='#D3D3D3', marker='.')\n",
    "plt.plot(x[arg_sort_x], multi_prs_pred[arg_sort_x], color='r', label='MultiPRS')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:09:56.698367Z",
     "start_time": "2024-02-21T23:09:56.131209Z"
    }
   },
   "id": "87d6e568b6b4dbc8",
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the MultiPRS model struggles to fit this heterogeneous data well. This is because it has to minimize the error \n",
    "with a single linear model across the entire domain. This is where the strengths of the Mixture of Experts (MoE) model come to shine."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd09789ffb087375"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mixture of Experts (MoE)\n",
    "\n",
    "The Mixture of Experts (MoE) model is a more flexible meta PRS model that can fit the data better than the MultiPRS model. The MoE model formulates the prediction as a weighted sum of the predictions of the pre-trained models, where the weights are determined by a gating network:\n",
    "\n",
    "$$\n",
    "y_{MoE}(i) = \\sum_{k=1}^{K} g_k(x_i) \\hat{y}_k(i)\n",
    "$$\n",
    "\n",
    "Here, $g_k(x_i)$ is the output of the gating model for the $i$-th observation and the $k$-th pre-trained model. In our case, the gating model is a linear model that takes the covariates as input and outputs the weights for the pre-trained models (i.e. it performs the equivalent of softmax regression). The weights are then used to combine the predictions of the pre-trained models to get the final prediction. The important thing to realize here is that the weights of the gating model depend on the input, which allows the MoE to select the best model for each input domain.\n",
    "\n",
    "Here's how the MoE model can be fit to the data using the `MoEPRS` class from the `moe` module:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85d526ed34dad1e3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the MultiPRS model:\n",
    "moe_model = MoEPRS(prs_data,\n",
    "                   expert_cols=model_names, # Specify the column names containing pre-trained model predictions\n",
    "                   gate_input_cols='x', # Specify the input columns for the gating model\n",
    "                   expert_add_intercept=False,\n",
    "                   gate_add_intercept=True)\n",
    "\n",
    "# Fit the MultiPRS model:\n",
    "moe_model.fit()\n",
    "\n",
    "# Visualize the fitted model predictions:\n",
    "moe_pred = moe_model.predict()\n",
    "\n",
    "# Plot the data and the pre-trained models:\n",
    "plt.scatter(x[arg_sort_x], y[arg_sort_x], color='#D3D3D3', marker='.')\n",
    "plt.plot(x[arg_sort_x], moe_pred[arg_sort_x], color='b', label='MoE')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:10:01.108449Z",
     "start_time": "2024-02-21T23:09:56.697132Z"
    }
   },
   "id": "475048c30c2173b3",
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's a lot better! The MoE model is able to fit the data much better by selectively using the pre-trained models in the regions \n",
    "where they fit the data well. To see what the gating model is doing, let's plot its weights as a function of the input variable `x`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45a430ef2c1e20f"
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the gating model weights:\n",
    "gate_weights = moe_model.predict_proba()\n",
    "\n",
    "# Plot the weights as a function of x:\n",
    "for i in range(gate_weights.shape[1]):\n",
    "    plt.plot(x[arg_sort_x], gate_weights[arg_sort_x, i], color=clrs[i], label=model_names[i])\n",
    "    \n",
    "# Plot the split point:\n",
    "plt.axvline(split_point, color='r', linestyle='--', label='Training split point')\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "plt.ylabel(\"Model weights\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T23:10:01.511684Z",
     "start_time": "2024-02-21T23:10:01.114581Z"
    }
   },
   "id": "c9269cb3c0970f87",
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we see here is that the gating model is correctly assigning high weights to Model 1 in the first half of the domain. However, \n",
    "for the second domain, because Model 2 does not fit the data perfectly, it tries to select a blend of the correct model (Model 2) with \n",
    "some of the random models in order to fit the data as well as possible."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46bc4224b2c0512d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
