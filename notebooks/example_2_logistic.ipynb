{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be214bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:47:53.328351Z",
     "start_time": "2024-09-24T00:47:52.894288Z"
    },
    "collapsed": true
   },
   "source": [
    "# Mixture of Experts (MoE) for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e54266",
   "metadata": {},
   "source": [
    "In this notebook, we will explore how to use the Mixture of Experts (MoE) framework to fit pre-trained \n",
    "generalized linear models to a simple 2 dimensional dataset. We will compare the MoE model to a simple logistic model that follows the `MultiPRS` framework (i.e. use all pre-trained models as covariates). \n",
    "\n",
    "To start, let's import the necessary libraries and modules that will be needed for this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba9976e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:03:42.238403Z",
     "start_time": "2024-09-24T01:03:42.214034Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sys.path.append(\"../model/\")\n",
    "from PRSDataset import PRSDataset\n",
    "from moe import MoEPRS\n",
    "from baseline_models import MultiPRS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "655bdc86",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438bba4",
   "metadata": {},
   "source": [
    "After importing the necessary libraries and modules, we will create a simple 2 dimensional dataset \n",
    "that displays heterogeneity in the relationship between the input variables `X` and the binary output variable `y`. To achieve this, we will assume that there are two underlying decision boundaries between \"cases\" and \"controls\" that can differ depending on the position of the input point `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09d7db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:53:42.442828Z",
     "start_time": "2024-09-24T00:53:42.256830Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of points\n",
    "N = 2000\n",
    "\n",
    "# Generate random points in polar coordinates\n",
    "r = np.random.normal(5, 1, N)\n",
    "theta = np.random.uniform(0, np.pi, N)\n",
    "\n",
    "# Convert to Cartesian coordinates\n",
    "x1 = r * np.cos(theta)\n",
    "x2 = r * np.sin(theta)\n",
    "\n",
    "# Add some noise\n",
    "x1 += np.random.normal(0, 0.5, N)\n",
    "x2 += np.random.normal(0, 0.5, N)\n",
    "\n",
    "# Assign labels\n",
    "labels = np.where(np.abs(x2) + np.abs(x1) > 4, 0, 1)\n",
    "markers = ['o', '+']\n",
    "\n",
    "# Plot the points\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(x1[labels == 0], x2[labels==0], marker=markers[0], label='Control')\n",
    "plt.scatter(x1[labels == 1], x2[labels==1], marker=markers[1], label='Case')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1d63a415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:58:32.801918Z",
     "start_time": "2024-09-24T00:58:32.779552Z"
    }
   },
   "source": [
    "The plot above shows the generated data. As we can see, the relationship between `x` and `y` cannot be \n",
    "easily modeled with a linear decision boundary, however, it can be well approximated by generalized \n",
    "linear models locally (within the two halves of the domain that we defined before).\n",
    "\n",
    "## Pre-trained Linear Models\n",
    "\n",
    "To make this setup more equivalent to the polygenic score (PGS) setting, let's assume that we are given access to a number \n",
    "of pre-trained linear models. Let's further assume that, similar to GWAS/PGS settings, those models were pre-trained on \n",
    "different subsets or domains of the data. To do that, we will implement the following procedure:\n",
    "\n",
    "1. Split the data into two (here we'll split based on the `x1` feature).\n",
    "2. Train a logistic model on each half of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea09ab62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:04:53.048552Z",
     "start_time": "2024-09-24T01:04:50.987056Z"
    }
   },
   "source": [
    "# ----------------------------------------------------------\n",
    "# Inference:\n",
    "\n",
    "# 1) split the data heuristically at x1=0.5\n",
    "\n",
    "train_x = np.vstack([x1, x2]).T\n",
    "\n",
    "train_data_1 = train_x[x1 < 0.5, :]\n",
    "train_label_1 = labels[x1 < 0.5]\n",
    "\n",
    "train_data_2 = train_x[x1 >= 0.5, :]\n",
    "train_label_2 = labels[x1 >= 0.5]\n",
    "\n",
    "# 2) Infer two sets of parameters\n",
    "\n",
    "lrm = LogisticRegression(fit_intercept=False, penalty=None)\n",
    "b1_hat = lrm.fit(train_data_1, train_label_1).coef_.T\n",
    "b2_hat = lrm.fit(train_data_2, train_label_2).coef_.T\n",
    "\n",
    "print(\"Inferred b1:\", b1_hat)\n",
    "print(\"Inferred b2:\", b2_hat)\n",
    "\n",
    "# 3) Get expert predictions on entire dataset:\n",
    "\n",
    "exp1_pred = train_x.dot(b1_hat)\n",
    "exp2_pred = train_x.dot(b2_hat)\n",
    "\n",
    "\n",
    "expert_predictions = np.concatenate([exp1_pred.reshape(-1, 1),\n",
    "                        exp2_pred.reshape(-1, 1)], axis=1)\n",
    "\n",
    "# 4) plot expert predictions:\n",
    "\n",
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=expit(exp1_pred).flatten(), vmin=0., vmax=1.)\n",
    "plt.title(\"Expert 1 predictions (trained on x1 >= 0.5)\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=expit(exp2_pred).flatten(), vmin=0., vmax=1.)\n",
    "plt.title(\"Expert 2 predictions (trained on x1 < 0.5)\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d4725430",
   "metadata": {},
   "source": [
    "## Meta models\n",
    "\n",
    "Now that we've generated the pre-trained models, we can use them to fit meta or ensemble model. A meta PRS model is a model that takes the predictions of simpler models and combines them in such a way that the combined model fits the data better than any of the simpler models individually. Before we explore the meta PRS models, \n",
    "let's aggregate our data in a `PRSDataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36cfe9ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:09:40.729593Z",
     "start_time": "2024-09-24T01:09:40.719282Z"
    }
   },
   "source": [
    "# Aggregate the data + model predictions in a dataframe:\n",
    "\n",
    "df = pd.DataFrame({'x1': x1, 'x2': x2, 'Model_1': exp1_pred.flatten(), 'Model_2': exp2_pred.flatten(), \n",
    "                   'y': labels})\n",
    "\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ca4fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:11:11.017932Z",
     "start_time": "2024-09-24T01:11:10.938198Z"
    }
   },
   "source": [
    "# Create a PRSDataset object:\n",
    "\n",
    "prs_data = PRSDataset(df, \n",
    "                      phenotype_col='y', # Specify the target column (equivalent to phenotype)\n",
    "                      prs_cols=['Model_1', 'Model_2'], # Specify the column names containing pre-trained model predictions\n",
    "                      covariates_cols=['x1', 'x2']) # Specify the column names containing the covariates (if any)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c82a02ab",
   "metadata": {},
   "source": [
    "### MultiPRS\n",
    "\n",
    "First, we will explore the MultiPRS model formulation, which takes the individual predictions of the pre-trained\n",
    "models and combines them linearly to get the final prediction. This can be done with the help of the `MultiPRS` model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1488f910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:16:22.766995Z",
     "start_time": "2024-09-24T01:16:22.381072Z"
    }
   },
   "source": [
    "# Create the MultiPRS model:\n",
    "multi_prs = MultiPRS(prs_data,\n",
    "                     expert_cols=['Model_1', 'Model_2'], # Specify the column names containing pre-trained model predictions\n",
    "                     covariates_cols=['x1', 'x2'])\n",
    "\n",
    "# Fit the MultiPRS model:\n",
    "multi_prs.fit()\n",
    "\n",
    "# Generate predictions (probabilities)\n",
    "multi_prs_pred = multi_prs.predict()\n",
    "\n",
    "# Plot the predictions:\n",
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=multi_prs_pred, vmin=0., vmax=1.)\n",
    "plt.title(\"MultiPRS predictions\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bcf3e883",
   "metadata": {},
   "source": [
    "As we can see, the MultiPRS model struggles to fit this heterogeneous data well. This is because it has to minimize the error with a single logistic model across the entire domain. This is where the strengths of the Mixture of Experts (MoE) model come to shine.\n",
    "\n",
    "### Mixture of Experts (MoE)\n",
    "\n",
    "The Mixture of Experts (MoE) model is a more flexible meta PRS model that can fit the data better than the MultiPRS model. The MoE model formulates the prediction as a weighted sum of the predictions of the pre-trained models, where the weights are determined by a gating network:\n",
    "\n",
    "$$\n",
    "y_{MoE}(i) = \\sum_{k=1}^{K} g_k(x_i) \\hat{y}_k(i)\n",
    "$$\n",
    "\n",
    "Here, $g_k(x_i)$ is the output of the gating model for the $i$-th observation and the $k$-th pre-trained model. In our case, the gating model is a linear model that takes the covariates as input and outputs the weights for the pre-trained models (i.e. it performs the equivalent of softmax regression). The weights are then used to combine the predictions of the pre-trained models to get the final prediction. The important thing to realize here is that the weights of the gating model depend on the input, which allows the MoE to select the best model for each input domain.\n",
    "\n",
    "Here's how the MoE model can be fit to the data using the `MoEPRS` class from the `moe` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7eb8dee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:21:49.234925Z",
     "start_time": "2024-09-24T01:21:48.557506Z"
    }
   },
   "source": [
    "# Create the MultiPRS model:\n",
    "moe_model = MoEPRS(prs_data,\n",
    "                   expert_cols=['Model_1', 'Model_2'], # Specify the column names containing pre-trained model predictions\n",
    "                   gate_input_cols=['x1', 'x2'], # Specify the input columns for the gating model\n",
    "                   expert_add_intercept=False,\n",
    "                   gate_add_intercept=True)\n",
    "\n",
    "# Fit the MoE model:\n",
    "moe_model.fit()\n",
    "\n",
    "# Generate the predictions (probabilities)\n",
    "moe_pred = moe_model.predict()\n",
    "\n",
    "# Plot the predictions:\n",
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=moe_pred, vmin=0., vmax=1.)\n",
    "plt.title(\"MoE predictions\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8f87d422",
   "metadata": {},
   "source": [
    "This looks a bit better! The MoE model is able to fit the data by selectively using the pre-trained models \n",
    "in the regions where they fit the data well. To see what the gating model is doing, \n",
    "let's plot its weights as a function of the input variables `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f692dee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:24:24.552392Z",
     "start_time": "2024-09-24T01:24:24.311028Z"
    }
   },
   "source": [
    "plt.scatter(train_x[:, 0], train_x[:, 1], c=moe_model.predict_proba()[:, 0].flatten(), vmin=0., vmax=1.)\n",
    "plt.title(\"Gate weight for model 1\")\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b845ad7",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
